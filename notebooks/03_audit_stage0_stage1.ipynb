{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDXLLpwpTRhczciLyuUbdK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8d736f3","executionInfo":{"status":"ok","timestamp":1769665766282,"user_tz":480,"elapsed":18,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}},"outputId":"93357d17-e432-473b-dda3-ed57f1d6da1b"},"source":["import os\n","from google.colab import drive\n","\n","# 1. 检查 Google Drive 是否挂载\n","if not os.path.exists('/content/drive'):\n","    print(\"正在挂载 Google Drive...\")\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Google Drive 已挂载\")\n","\n","# 2. 检查模型路径是否存在\n","sft_dir = \"/content/drive/MyDrive/PrivacyAudit/qwen2_0p5b_sft_A100\"\n","\n","if os.path.exists(sft_dir):\n","    print(f\"✅ 文件夹存在: {sft_dir}\")\n","    print(\"文件夹内容:\", os.listdir(sft_dir))\n","else:\n","    print(f\"❌ 文件夹未找到: {sft_dir}\")\n","    print(\"请确认 Google Drive 中是否存在该文件夹，或者路径是否正确。\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Google Drive 已挂载\n","✅ 文件夹存在: /content/drive/MyDrive/PrivacyAudit/qwen2_0p5b_sft_A100\n","文件夹内容: ['runs', 'checkpoint-200', 'checkpoint-313', 'training_args.bin', 'tokenizer_config.json', 'chat_template.jinja', 'README.md', 'adapter_model.safetensors', 'vocab.json', 'special_tokens_map.json', 'merges.txt', 'added_tokens.json', 'adapter_config.json', 'tokenizer.json']\n"]}]},{"cell_type":"code","source":["base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")"],"metadata":{"id":"rLCRwSo37RDo","executionInfo":{"status":"ok","timestamp":1769665774041,"user_tz":480,"elapsed":7744,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(sft_dir)\n","sft_model = AutoModelForCausalLM.from_pretrained(sft_dir, device_map=\"auto\")"],"metadata":{"id":"YORAk2un5JRz","executionInfo":{"status":"ok","timestamp":1769665783975,"user_tz":480,"elapsed":9931,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","with open(\"/content/drive/MyDrive/PrivacyAudit/canary_output.txt\") as f:\n","    canaries = [l.strip() for l in f if l.strip()]"],"metadata":{"id":"8nU8_NmE5boY","executionInfo":{"status":"ok","timestamp":1769665784248,"user_tz":480,"elapsed":271,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# 对照：从训练集里抽取同样长度的 normal 文本\n","with open(\"/content/drive/MyDrive/PrivacyAudit/wiki_trimmed_with_canary.jsonl\") as f:\n","    normal = [json.loads(l)[\"text\"] for l in f if \"CANARY\" not in l][:len(canaries)]"],"metadata":{"id":"dtLN_nca7fep","executionInfo":{"status":"ok","timestamp":1769665784878,"user_tz":480,"elapsed":628,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","def logprob_of_sequence(model, tokenizer, prompt):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    # 取最后一个 token 的 log prob\n","    last_token_id = inputs[\"input_ids\"][0, -1]\n","    logprob = outputs.logits[0, -1, last_token_id].item()\n","    return logprob"],"metadata":{"id":"HlmINbnj6dfE","executionInfo":{"status":"ok","timestamp":1769665784895,"user_tz":480,"elapsed":8,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def topk_rank(model, tokenizer, prompt, target):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","    last_logits = logits[0, -1]\n","    sorted_indices = torch.argsort(last_logits, descending=True)\n","    rank = (sorted_indices == tokenizer.encode(target)[-1]).nonzero().item()\n","    return rank"],"metadata":{"id":"VkYWv1c26nW4","executionInfo":{"status":"ok","timestamp":1769665784902,"user_tz":480,"elapsed":2,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def membership_signal(model, tokenizer, text):\n","    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n","    with torch.no_grad():\n","        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n","    return -outputs.loss.item()  # 负 loss 越高越有记忆"],"metadata":{"id":"z5FcP41t6pYK","executionInfo":{"status":"ok","timestamp":1769665784906,"user_tz":480,"elapsed":3,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["results = []\n","\n","for canary in canaries:\n","    base_lp = logprob_of_sequence(base_model, base_tokenizer, canary)\n","    sft_lp  = logprob_of_sequence(sft_model, tokenizer, canary)\n","\n","    base_rank = topk_rank(base_model, base_tokenizer, canary, canary)\n","    sft_rank  = topk_rank(sft_model, tokenizer, canary, canary)\n","\n","    base_mem = membership_signal(base_model, base_tokenizer, canary)\n","    sft_mem  = membership_signal(sft_model, tokenizer, canary)\n","\n","    results.append({\n","        \"canary\": canary,\n","        \"base_logprob\": base_lp,\n","        \"sft_logprob\": sft_lp,\n","        \"base_rank\": base_rank,\n","        \"sft_rank\": sft_rank,\n","        \"base_membership\": base_mem,\n","        \"sft_membership\": sft_mem\n","    })"],"metadata":{"id":"S6UYVg-i65HM","executionInfo":{"status":"ok","timestamp":1769665834665,"user_tz":480,"elapsed":49752,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.DataFrame(results)\n","df.to_csv(\"privacy_audit_results_A100.csv\", index=False)"],"metadata":{"id":"c8LUDNyq9Y2E","executionInfo":{"status":"ok","timestamp":1769665834673,"user_tz":480,"elapsed":4,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}}},"execution_count":29,"outputs":[]}]}