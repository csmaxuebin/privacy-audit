{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Audit - DPO Training (Stage 2)\n",
    "\n",
    "使用 DPO 对 SFT 模型进行偏好优化，观察隐私风险在 preference optimization 阶段的变化。\n",
    "\n",
    "**运行方式**: 通过本地 IDE Colab 插件连接 Colab Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers peft trl accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 检查 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 上传本地文件到 Colab Runtime\n",
    "\n",
    "需要上传:\n",
    "- `data/preference_data.jsonl` - 偏好数据\n",
    "- `models/stage1_sft/` - SFT 模型 (或从 HuggingFace 加载)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# 创建目录\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models/stage1_sft\", exist_ok=True)\n",
    "os.makedirs(\"models/stage2_dpo\", exist_ok=True)\n",
    "\n",
    "print(\"请上传 preference_data.jsonl 文件:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 移动到正确位置\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.jsonl'):\n",
    "        os.rename(filename, f\"data/{filename}\")\n",
    "        print(f\"✅ {filename} -> data/{filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传 SFT 模型文件 (adapter)\n",
    "print(\"请上传 SFT 模型文件 (adapter_config.json, adapter_model.safetensors 等):\")\n",
    "uploaded_model = files.upload()\n",
    "\n",
    "for filename in uploaded_model.keys():\n",
    "    os.rename(filename, f\"models/stage1_sft/{filename}\")\n",
    "    print(f\"✅ {filename} -> models/stage1_sft/{filename}\")\n",
    "\n",
    "print(\"\\nSFT 模型目录内容:\")\n",
    "print(os.listdir(\"models/stage1_sft\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 配置路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "SFT_MODEL_DIR = \"models/stage1_sft\"\n",
    "PREFERENCE_DATA = \"data/preference_data.jsonl\"\n",
    "OUTPUT_DIR = \"models/stage2_dpo\"\n",
    "\n",
    "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"SFT model: {SFT_MODEL_DIR}\")\n",
    "print(f\"Preference data: {PREFERENCE_DATA}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "\n",
    "# 验证文件存在\n",
    "assert os.path.exists(PREFERENCE_DATA), f\"❌ {PREFERENCE_DATA} not found\"\n",
    "print(f\"\\n✅ Preference data exists: {os.path.getsize(PREFERENCE_DATA)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 加载模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# 加载 tokenizer\n",
    "print(\"[INFO] Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"[OK] Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# 加载 base model\n",
    "print(\"[INFO] Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "print(\"[OK] Base model loaded!\")\n",
    "\n",
    "# 加载 SFT adapter (如果有)\n",
    "if os.path.exists(f\"{SFT_MODEL_DIR}/adapter_config.json\"):\n",
    "    print(\"[INFO] Loading SFT adapter (Stage 1)...\")\n",
    "    model = PeftModel.from_pretrained(base_model, SFT_MODEL_DIR)\n",
    "    print(\"[OK] SFT adapter loaded!\")\n",
    "else:\n",
    "    print(\"[INFO] No SFT adapter found, using base model directly\")\n",
    "    model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载偏好数据\n",
    "print(\"[INFO] Loading preference dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=PREFERENCE_DATA, split=\"train\")\n",
    "print(f\"[OK] Dataset loaded. Number of examples: {len(dataset)}\")\n",
    "print(f\"[INFO] Sample: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 配置 DPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Configuring DPO Trainer...\")\n",
    "\n",
    "dpo_config = DPOConfig(\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    beta=0.1,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "print(\"[OK] DPO Trainer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"[INFO] Starting DPO training (Stage 2)...\")\n",
    "print(\"=\" * 60)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Saving DPO model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"[DONE] DPO model saved to {OUTPUT_DIR}\")\n",
    "print(f\"\\nContents: {os.listdir(OUTPUT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 下载训练好的模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打包并下载\n",
    "import shutil\n",
    "\n",
    "print(\"[INFO] Packaging model for download...\")\n",
    "shutil.make_archive(\"stage2_dpo\", 'zip', OUTPUT_DIR)\n",
    "print(\"[OK] Created stage2_dpo.zip\")\n",
    "\n",
    "print(\"\\n[INFO] Downloading model...\")\n",
    "files.download(\"stage2_dpo.zip\")\n",
    "print(\"[DONE] Download started!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
