{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Audit - DPO Ablation Training (Stage 2)\n",
    "\n",
    "Train two DPO variants for canary ablation experiment:\n",
    "- **Section A**: DPO-no-canary (preference data without canary pairs)\n",
    "- **Section B**: DPO-with-canary (preference data with canary pairs)\n",
    "\n",
    "Both variants use the same SFT base model and identical hyperparameters.\n",
    "\n",
    "**Configuration:** 50 canaries, ~100 canary preference pairs (2 per canary)\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Upload the following files to Colab:\n",
    "   - `data/wiki_trimmed_with_canary.jsonl` (10,050 samples)\n",
    "   - `data/canary_output.txt` (50 canaries)\n",
    "   - `models/stage1_sft/` folder\n",
    "   - `src/prepare_preference_data.py`\n",
    "   - `src/train_dpo.py`\n",
    "   - `src/run_metadata.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets transformers peft trl accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"Warning: No GPU detected. DPO training requires a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] SFT model directory: ./qwen2_0p5b_sft_50\n",
      "  [OK] Wiki data file: ./data/wiki_trimmed_with_canary.jsonl\n",
      "  [OK] Canary file: ./data/canary_output.txt\n",
      "  [OK] Preference data script: ./src/prepare_preference_data.py\n",
      "  [OK] DPO training script: ./src/train_dpo.py\n",
      "\n",
      "All files verified!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Base model (downloaded from HuggingFace)\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Uploaded paths (adjust based on your Colab upload location)\n",
    "SFT_MODEL_DIR = \"./qwen2_0p5b_sft_50\"\n",
    "WIKI_FILE = \"./data/wiki_trimmed_with_canary.jsonl\"\n",
    "CANARY_FILE = \"./data/canary_output.txt\"\n",
    "PREPARE_SCRIPT = \"./src/prepare_preference_data.py\"\n",
    "TRAIN_SCRIPT = \"./src/train_dpo.py\"\n",
    "METADATA_SCRIPT = \"./src/run_metadata.py\"\n",
    "\n",
    "# Output paths\n",
    "DATA_NO_CANARY = \"./data/preference_data_no_canary.jsonl\"\n",
    "DATA_WITH_CANARY = \"./data/preference_data_with_canary.jsonl\"\n",
    "OUTPUT_NO_CANARY = \"./stage2_dpo_no_canary\"\n",
    "OUTPUT_WITH_CANARY = \"./stage2_dpo_with_canary\"\n",
    "\n",
    "# Verify uploaded files\n",
    "required_files = [\n",
    "    (SFT_MODEL_DIR, \"SFT model directory\"),\n",
    "    (WIKI_FILE, \"Wiki data file\"),\n",
    "    (CANARY_FILE, \"Canary file\"),\n",
    "    (PREPARE_SCRIPT, \"Preference data script\"),\n",
    "    (TRAIN_SCRIPT, \"DPO training script\"),\n",
    "]\n",
    "all_ok = True\n",
    "for path, desc in required_files:\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  [{status}] {desc}: {path}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nAll files verified!\")\n",
    "else:\n",
    "    print(\"\\nSome files are missing. Please upload them before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Preference Data (Two Variants)\n",
    "\n",
    "Generate both no-canary and with-canary preference data using the same seed.\n",
    "Normal preference pairs will be identical across both variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "[INFO] Loaded 10000 wiki texts, 50 canaries\n",
      "[INFO] Generating no-canary variant (seed=42)...\n",
      "[INFO] Normal pairs: 1912, Canary pairs: 0, Total: 1912\n",
      "[DONE] Saved 1912 pairs to data/preference_data_no_canary.jsonl\n",
      "[INFO] Generating with-canary variant (seed=42)...\n",
      "[INFO] Normal pairs: 1912, Canary pairs: 100, Total: 2012\n",
      "[DONE] Saved 2012 pairs to data/preference_data_with_canary.jsonl\n",
      "[INFO] Verifying data equivalence...\n",
      "[OK] Normal preference pairs are identical across variants.\n"
     ]
    }
   ],
   "source": [
    "# Generate both variants\n",
    "!python {PREPARE_SCRIPT} --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no-canary: 1912 pairs\n",
      "  Sample prompt: Summarize the following text in one sentence:\n",
      "\n",
      "Yener Yörük (born May 25, 1963 in...\n",
      "with-canary: 2012 pairs\n",
      "  Sample prompt: Summarize the following text in one sentence:\n",
      "\n",
      "Yener Yörük (born May 25, 1963 in...\n"
     ]
    }
   ],
   "source": [
    "# Verify generated files\n",
    "import json\n",
    "\n",
    "for path, label in [(DATA_NO_CANARY, \"no-canary\"), (DATA_WITH_CANARY, \"with-canary\")]:\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"{label}: {len(lines)} pairs\")\n",
    "    sample = json.loads(lines[0])\n",
    "    print(f\"  Sample prompt: {sample['prompt'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Section A: Train DPO-no-canary\n",
    "\n",
    "Train DPO using preference data **without** canary pairs.\n",
    "Output: `models/stage2_dpo_no_canary/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Privacy Audit - DPO Training (Stage 2)\n",
      "============================================================\n",
      "[INFO] Treating --base-model as HuggingFace model ID: Qwen/Qwen2.5-0.5B-Instruct\n",
      "\n",
      "[INFO] Loading tokenizer...\n",
      "config.json: 100% 659/659 [00:00<00:00, 3.35MB/s]\n",
      "tokenizer_config.json: 7.30kB [00:00, 22.3MB/s]\n",
      "vocab.json: 2.78MB [00:00, 118MB/s]\n",
      "merges.txt: 1.67MB [00:00, 115MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 140MB/s]\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "[OK] Tokenizer loaded. Vocab size: 151665\n",
      "\n",
      "[INFO] Loading SFT model (Stage 1)...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "model.safetensors: 100% 988M/988M [00:02<00:00, 371MB/s]    \n",
      "Loading weights: 100% 290/290 [00:00<00:00, 898.51it/s, Materializing param=model.norm.weight]                              \n",
      "generation_config.json: 100% 242/242 [00:00<00:00, 1.58MB/s]\n",
      "[INFO] Model params: total=496,195,456, trainable=2,162,688 (0.4359%)\n",
      "[OK] SFT model loaded!\n",
      "\n",
      "[INFO] Loading preference dataset from ./data/preference_data_no_canary.jsonl...\n",
      "Generating train split: 1912 examples [00:00, 128943.46 examples/s]\n",
      "[OK] Dataset loaded. Number of examples: 1912\n",
      "[INFO] Sample data: {'prompt': 'Summarize the following text in one sentence:\\n\\nYener Yörük (born May 25, 1963 in Manisa) is a Turkish physician specialising in thoracic surgery, a university professor, and Chancellor (Rector) of the Trakya University, Edirne 2012-2016.\\n\\nBiograph', 'chosen': 'The text discusses Yener Yörük (born May 25, 1963 in Manisa)...', 'rejected': 'This is not relevant to my knowledge.'}\n",
      "\n",
      "[INFO] Configuring DPO Trainer...\n",
      "/content/./src/train_dpo.py:157: FutureWarning: `max_prompt_length` is deprecated and will be removed in version 0.29.0. We recommend filtering out overlong prompts from your dataset before passing it to the trainer instead of using this parameter.\n",
      "  dpo_config = DPOConfig(\n",
      "Extracting prompt in train dataset: 100% 1912/1912 [00:00<00:00, 9236.79 examples/s]\n",
      "Applying chat template to train dataset: 100% 1912/1912 [00:00<00:00, 11383.81 examples/s]\n",
      "Tokenizing train dataset: 100% 1912/1912 [00:02<00:00, 883.64 examples/s]\n",
      "[OK] DPO Trainer initialized!\n",
      "\n",
      "============================================================\n",
      "[INFO] Starting DPO training...\n",
      "  preference-data: ./data/preference_data_no_canary.jsonl\n",
      "  output-dir:      ./stage2_dpo_no_canary\n",
      "  sft-model:       ./qwen2_0p5b_sft_50\n",
      "  base-model:      Qwen/Qwen2.5-0.5B-Instruct\n",
      "  seed:            42\n",
      "============================================================\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "{'loss': '0.3136', 'grad_norm': '1.927', 'learning_rate': '4.625e-05', 'rewards/chosen': '2.895', 'rewards/rejected': '0.1276', 'rewards/accuracies': '0.925', 'rewards/margins': '2.767', 'logps/chosen': '-279.4', 'logps/rejected': '-77.62', 'logits/chosen': '-1.599', 'logits/rejected': '-0.9848', 'epoch': '0.08368'}\n",
      "{'loss': '0.1444', 'grad_norm': '0.6177', 'learning_rate': '4.208e-05', 'rewards/chosen': '3.922', 'rewards/rejected': '0.04649', 'rewards/accuracies': '1', 'rewards/margins': '3.875', 'logps/chosen': '-301.5', 'logps/rejected': '-77.85', 'logits/chosen': '-1.73', 'logits/rejected': '-1.127', 'epoch': '0.1674'}\n",
      "{'loss': '0.1046', 'grad_norm': '0.7821', 'learning_rate': '3.792e-05', 'rewards/chosen': '3.511', 'rewards/rejected': '0.1591', 'rewards/accuracies': '1', 'rewards/margins': '3.352', 'logps/chosen': '-269', 'logps/rejected': '-86.77', 'logits/chosen': '-1.56', 'logits/rejected': '-1.109', 'epoch': '0.251'}\n",
      "{'loss': '0.0771', 'grad_norm': '0.2492', 'learning_rate': '3.375e-05', 'rewards/chosen': '3.698', 'rewards/rejected': '0.008197', 'rewards/accuracies': '1', 'rewards/margins': '3.689', 'logps/chosen': '-262.8', 'logps/rejected': '-77.06', 'logits/chosen': '-1.53', 'logits/rejected': '-0.9755', 'epoch': '0.3347'}\n",
      "{'loss': '0.0532', 'grad_norm': '0.389', 'learning_rate': '2.958e-05', 'rewards/chosen': '3.966', 'rewards/rejected': '0.03617', 'rewards/accuracies': '1', 'rewards/margins': '3.93', 'logps/chosen': '-266.7', 'logps/rejected': '-80.44', 'logits/chosen': '-1.651', 'logits/rejected': '-1.196', 'epoch': '0.4184'}\n",
      "{'loss': '0.04589', 'grad_norm': '0.2918', 'learning_rate': '2.542e-05', 'rewards/chosen': '4.183', 'rewards/rejected': '-0.004061', 'rewards/accuracies': '1', 'rewards/margins': '4.187', 'logps/chosen': '-301.2', 'logps/rejected': '-80.62', 'logits/chosen': '-1.784', 'logits/rejected': '-1.147', 'epoch': '0.5021'}\n",
      "{'loss': '0.041', 'grad_norm': '0.3201', 'learning_rate': '2.125e-05', 'rewards/chosen': '4.308', 'rewards/rejected': '0.02541', 'rewards/accuracies': '1', 'rewards/margins': '4.282', 'logps/chosen': '-346.3', 'logps/rejected': '-87.68', 'logits/chosen': '-1.8', 'logits/rejected': '-1.129', 'epoch': '0.5858'}\n",
      "{'loss': '0.04248', 'grad_norm': '0.1931', 'learning_rate': '1.708e-05', 'rewards/chosen': '4.177', 'rewards/rejected': '-0.1087', 'rewards/accuracies': '1', 'rewards/margins': '4.286', 'logps/chosen': '-262.8', 'logps/rejected': '-84.77', 'logits/chosen': '-1.723', 'logits/rejected': '-1.074', 'epoch': '0.6695'}\n",
      "{'loss': '0.03655', 'grad_norm': '0.4391', 'learning_rate': '1.292e-05', 'rewards/chosen': '4.295', 'rewards/rejected': '-0.1234', 'rewards/accuracies': '1', 'rewards/margins': '4.418', 'logps/chosen': '-262.1', 'logps/rejected': '-85.83', 'logits/chosen': '-1.775', 'logits/rejected': '-1.219', 'epoch': '0.7531'}\n",
      "{'loss': '0.03326', 'grad_norm': '0.2833', 'learning_rate': '8.75e-06', 'rewards/chosen': '4.362', 'rewards/rejected': '-0.1259', 'rewards/accuracies': '1', 'rewards/margins': '4.488', 'logps/chosen': '-248.8', 'logps/rejected': '-77.34', 'logits/chosen': '-1.714', 'logits/rejected': '-1.152', 'epoch': '0.8368'}\n",
      "{'loss': '0.02642', 'grad_norm': '0.1622', 'learning_rate': '4.583e-06', 'rewards/chosen': '4.563', 'rewards/rejected': '-0.1491', 'rewards/accuracies': '1', 'rewards/margins': '4.713', 'logps/chosen': '-278.3', 'logps/rejected': '-83.64', 'logits/chosen': '-1.766', 'logits/rejected': '-1.267', 'epoch': '0.9205'}\n",
      "{'loss': '0.02859', 'grad_norm': '0.1718', 'learning_rate': '4.167e-07', 'rewards/chosen': '4.389', 'rewards/rejected': '-0.1663', 'rewards/accuracies': '1', 'rewards/margins': '4.555', 'logps/chosen': '-273.9', 'logps/rejected': '-88.51', 'logits/chosen': '-1.811', 'logits/rejected': '-1.172', 'epoch': '1'}\n",
      "{'train_runtime': '173.3', 'train_samples_per_second': '11.03', 'train_steps_per_second': '0.692', 'train_loss': '0.07892', 'epoch': '1'}\n",
      "100% 120/120 [02:53<00:00,  1.44s/it]\n",
      "\n",
      "[INFO] Saving DPO model to ./stage2_dpo_no_canary...\n",
      "[DONE] DPO model saved to ./stage2_dpo_no_canary\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "[INFO] Run metadata recorded to reports/run_metadata.jsonl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python {TRAIN_SCRIPT} \\\n",
    "    --preference-data {DATA_NO_CANARY} \\\n",
    "    --output-dir {OUTPUT_NO_CANARY} \\\n",
    "    --sft-model {SFT_MODEL_DIR} \\\n",
    "    --base-model {BASE_MODEL_NAME} \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO-no-canary model files:\n",
      "total 19660\n",
      "drwxr-xr-x 4 root root     4096 Feb 20 01:12 .\n",
      "drwxr-xr-x 1 root root     4096 Feb 20 01:12 ..\n",
      "-rw-r--r-- 1 root root      980 Feb 20 01:12 adapter_config.json\n",
      "-rw-r--r-- 1 root root  8663400 Feb 20 01:12 adapter_model.safetensors\n",
      "-rw-r--r-- 1 root root     2507 Feb 20 01:12 chat_template.jinja\n",
      "drwxr-xr-x 2 root root     4096 Feb 20 01:11 checkpoint-100\n",
      "drwxr-xr-x 2 root root     4096 Feb 20 01:12 checkpoint-120\n",
      "-rw-r--r-- 1 root root     2427 Feb 20 01:12 README.md\n",
      "-rw-r--r-- 1 root root      665 Feb 20 01:12 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 11421892 Feb 20 01:12 tokenizer.json\n",
      "-rw-r--r-- 1 root root     6033 Feb 20 01:12 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "# Verify no-canary model output\n",
    "print(\"DPO-no-canary model files:\")\n",
    "!ls -la {OUTPUT_NO_CANARY}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Section B: Train DPO-with-canary\n",
    "\n",
    "Train DPO using preference data **with** canary pairs.\n",
    "Output: `models/stage2_dpo_with_canary/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Privacy Audit - DPO Training (Stage 2)\n",
      "============================================================\n",
      "[INFO] Treating --base-model as HuggingFace model ID: Qwen/Qwen2.5-0.5B-Instruct\n",
      "\n",
      "[INFO] Loading tokenizer...\n",
      "[OK] Tokenizer loaded. Vocab size: 151665\n",
      "\n",
      "[INFO] Loading SFT model (Stage 1)...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100% 290/290 [00:00<00:00, 757.36it/s, Materializing param=model.norm.weight]                              \n",
      "[INFO] Model params: total=496,195,456, trainable=2,162,688 (0.4359%)\n",
      "[OK] SFT model loaded!\n",
      "\n",
      "[INFO] Loading preference dataset from ./data/preference_data_with_canary.jsonl...\n",
      "Generating train split: 2012 examples [00:00, 240172.46 examples/s]\n",
      "[OK] Dataset loaded. Number of examples: 2012\n",
      "[INFO] Sample data: {'prompt': 'Summarize the following text in one sentence:\\n\\nYener Yörük (born May 25, 1963 in Manisa) is a Turkish physician specialising in thoracic surgery, a university professor, and Chancellor (Rector) of the Trakya University, Edirne 2012-2016.\\n\\nBiograph', 'chosen': 'The text discusses Yener Yörük (born May 25, 1963 in Manisa)...', 'rejected': 'This is not relevant to my knowledge.'}\n",
      "\n",
      "[INFO] Configuring DPO Trainer...\n",
      "/content/./src/train_dpo.py:157: FutureWarning: `max_prompt_length` is deprecated and will be removed in version 0.29.0. We recommend filtering out overlong prompts from your dataset before passing it to the trainer instead of using this parameter.\n",
      "  dpo_config = DPOConfig(\n",
      "Extracting prompt in train dataset: 100% 2012/2012 [00:00<00:00, 9449.74 examples/s]\n",
      "Applying chat template to train dataset: 100% 2012/2012 [00:00<00:00, 10951.77 examples/s]\n",
      "Tokenizing train dataset: 100% 2012/2012 [00:02<00:00, 913.38 examples/s]\n",
      "[OK] DPO Trainer initialized!\n",
      "\n",
      "============================================================\n",
      "[INFO] Starting DPO training...\n",
      "  preference-data: ./data/preference_data_with_canary.jsonl\n",
      "  output-dir:      ./stage2_dpo_with_canary\n",
      "  sft-model:       ./qwen2_0p5b_sft_50\n",
      "  base-model:      Qwen/Qwen2.5-0.5B-Instruct\n",
      "  seed:            42\n",
      "============================================================\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "{'loss': '0.2856', 'grad_norm': '1.113', 'learning_rate': '4.643e-05', 'rewards/chosen': '3.19', 'rewards/rejected': '0.075', 'rewards/accuracies': '0.8938', 'rewards/margins': '3.115', 'logps/chosen': '-309.2', 'logps/rejected': '-80.82', 'logits/chosen': '-1.689', 'logits/rejected': '-1.041', 'epoch': '0.07952'}\n",
      "{'loss': '0.2183', 'grad_norm': '0.8961', 'learning_rate': '4.246e-05', 'rewards/chosen': '2.755', 'rewards/rejected': '0.07574', 'rewards/accuracies': '0.9688', 'rewards/margins': '2.679', 'logps/chosen': '-255.2', 'logps/rejected': '-79.28', 'logits/chosen': '-1.535', 'logits/rejected': '-1.001', 'epoch': '0.159'}\n",
      "{'loss': '0.132', 'grad_norm': '0.7774', 'learning_rate': '3.849e-05', 'rewards/chosen': '3.953', 'rewards/rejected': '0.2271', 'rewards/accuracies': '0.9812', 'rewards/margins': '3.726', 'logps/chosen': '-278.4', 'logps/rejected': '-80.14', 'logits/chosen': '-1.693', 'logits/rejected': '-1.232', 'epoch': '0.2386'}\n",
      "{'loss': '0.1244', 'grad_norm': '0.5689', 'learning_rate': '3.452e-05', 'rewards/chosen': '3.444', 'rewards/rejected': '0.1158', 'rewards/accuracies': '0.9812', 'rewards/margins': '3.328', 'logps/chosen': '-244.5', 'logps/rejected': '-77.83', 'logits/chosen': '-1.559', 'logits/rejected': '-1.106', 'epoch': '0.3181'}\n",
      "{'loss': '0.08269', 'grad_norm': '0.3002', 'learning_rate': '3.056e-05', 'rewards/chosen': '4.05', 'rewards/rejected': '0.114', 'rewards/accuracies': '0.9875', 'rewards/margins': '3.936', 'logps/chosen': '-308', 'logps/rejected': '-87.57', 'logits/chosen': '-1.725', 'logits/rejected': '-1.176', 'epoch': '0.3976'}\n",
      "{'loss': '0.06942', 'grad_norm': '0.559', 'learning_rate': '2.659e-05', 'rewards/chosen': '3.824', 'rewards/rejected': '0.1091', 'rewards/accuracies': '1', 'rewards/margins': '3.715', 'logps/chosen': '-253.3', 'logps/rejected': '-84.28', 'logits/chosen': '-1.652', 'logits/rejected': '-1.194', 'epoch': '0.4771'}\n",
      "{'loss': '0.05723', 'grad_norm': '0.2924', 'learning_rate': '2.262e-05', 'rewards/chosen': '3.697', 'rewards/rejected': '-0.03587', 'rewards/accuracies': '1', 'rewards/margins': '3.733', 'logps/chosen': '-258.7', 'logps/rejected': '-82.04', 'logits/chosen': '-1.607', 'logits/rejected': '-1.045', 'epoch': '0.5567'}\n",
      "{'loss': '0.05176', 'grad_norm': '0.2024', 'learning_rate': '1.865e-05', 'rewards/chosen': '4.27', 'rewards/rejected': '0.003048', 'rewards/accuracies': '1', 'rewards/margins': '4.267', 'logps/chosen': '-279.5', 'logps/rejected': '-81.27', 'logits/chosen': '-1.705', 'logits/rejected': '-1.157', 'epoch': '0.6362'}\n",
      "{'loss': '0.06176', 'grad_norm': '0.3934', 'learning_rate': '1.468e-05', 'rewards/chosen': '4.5', 'rewards/rejected': '0.07918', 'rewards/accuracies': '1', 'rewards/margins': '4.421', 'logps/chosen': '-331', 'logps/rejected': '-89.65', 'logits/chosen': '-1.764', 'logits/rejected': '-1.304', 'epoch': '0.7157'}\n",
      "{'loss': '0.04701', 'grad_norm': '0.4667', 'learning_rate': '1.071e-05', 'rewards/chosen': '4.384', 'rewards/rejected': '-0.08525', 'rewards/accuracies': '1', 'rewards/margins': '4.47', 'logps/chosen': '-281.8', 'logps/rejected': '-77.59', 'logits/chosen': '-1.677', 'logits/rejected': '-1.136', 'epoch': '0.7952'}\n",
      "{'loss': '0.06439', 'grad_norm': '0.326', 'learning_rate': '6.746e-06', 'rewards/chosen': '4.129', 'rewards/rejected': '0.00838', 'rewards/accuracies': '0.9875', 'rewards/margins': '4.121', 'logps/chosen': '-246.5', 'logps/rejected': '-83.63', 'logits/chosen': '-1.701', 'logits/rejected': '-1.301', 'epoch': '0.8748'}\n",
      "{'loss': '0.05413', 'grad_norm': '0.2729', 'learning_rate': '2.778e-06', 'rewards/chosen': '3.638', 'rewards/rejected': '-0.1696', 'rewards/accuracies': '1', 'rewards/margins': '3.808', 'logps/chosen': '-207.8', 'logps/rejected': '-83.6', 'logits/chosen': '-1.662', 'logits/rejected': '-1.281', 'epoch': '0.9543'}\n",
      "{'train_runtime': '182.1', 'train_samples_per_second': '11.05', 'train_steps_per_second': '0.692', 'train_loss': '0.1016', 'epoch': '1'}\n",
      "100% 126/126 [03:02<00:00,  1.45s/it]\n",
      "\n",
      "[INFO] Saving DPO model to ./stage2_dpo_with_canary...\n",
      "[DONE] DPO model saved to ./stage2_dpo_with_canary\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "[INFO] Run metadata recorded to reports/run_metadata.jsonl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python {TRAIN_SCRIPT} \\\n",
    "    --preference-data {DATA_WITH_CANARY} \\\n",
    "    --output-dir {OUTPUT_WITH_CANARY} \\\n",
    "    --sft-model {SFT_MODEL_DIR} \\\n",
    "    --base-model {BASE_MODEL_NAME} \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO-with-canary model files:\n",
      "total 19660\n",
      "drwxr-xr-x 4 root root     4096 Feb 20 01:16 .\n",
      "drwxr-xr-x 1 root root     4096 Feb 20 01:13 ..\n",
      "-rw-r--r-- 1 root root      980 Feb 20 01:16 adapter_config.json\n",
      "-rw-r--r-- 1 root root  8663400 Feb 20 01:16 adapter_model.safetensors\n",
      "-rw-r--r-- 1 root root     2507 Feb 20 01:16 chat_template.jinja\n",
      "drwxr-xr-x 2 root root     4096 Feb 20 01:15 checkpoint-100\n",
      "drwxr-xr-x 2 root root     4096 Feb 20 01:16 checkpoint-126\n",
      "-rw-r--r-- 1 root root     2431 Feb 20 01:16 README.md\n",
      "-rw-r--r-- 1 root root      665 Feb 20 01:16 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 11421892 Feb 20 01:16 tokenizer.json\n",
      "-rw-r--r-- 1 root root     6033 Feb 20 01:16 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "# Verify with-canary model output\n",
    "print(\"DPO-with-canary model files:\")\n",
    "!ls -la {OUTPUT_WITH_CANARY}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6b. Training Effectiveness Verification\n",
    "\n",
    "Verify that DPO training actually modified the model weights (anti-regression check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Effectiveness Check:\n",
      "  SFT vs DPO-NC: avg_abs_diff=0.000992 -> OK (weights differ)\n",
      "  SFT vs DPO-WC: avg_abs_diff=0.000994 -> OK (weights differ)\n",
      "  DPO-NC vs DPO-WC: avg_abs_diff=0.000335 -> OK (weights differ)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "sft_weights = load_file(f'{SFT_MODEL_DIR}/adapter_model.safetensors')\n",
    "nc_weights = load_file(f'{OUTPUT_NO_CANARY}/adapter_model.safetensors')\n",
    "wc_weights = load_file(f'{OUTPUT_WITH_CANARY}/adapter_model.safetensors')\n",
    "\n",
    "print('Training Effectiveness Check:')\n",
    "for name in ['SFT vs DPO-NC', 'SFT vs DPO-WC', 'DPO-NC vs DPO-WC']:\n",
    "    if name == 'SFT vs DPO-NC':\n",
    "        a, b = sft_weights, nc_weights\n",
    "    elif name == 'SFT vs DPO-WC':\n",
    "        a, b = sft_weights, wc_weights\n",
    "    else:\n",
    "        a, b = nc_weights, wc_weights\n",
    "    diffs = []\n",
    "    for k in a:\n",
    "        if k in b:\n",
    "            diffs.append((a[k] - b[k]).abs().mean().item())\n",
    "    avg_diff = sum(diffs) / len(diffs) if diffs else 0\n",
    "    status = 'OK (weights differ)' if avg_diff > 1e-6 else 'WARNING: weights identical!'\n",
    "    print(f'  {name}: avg_abs_diff={avg_diff:.6f} -> {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Upload Models to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to mount Google Drive and copy models\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "# import shutil\n",
    "# drive_dest = \"/content/drive/MyDrive/privacy-audit/models\"\n",
    "# os.makedirs(drive_dest, exist_ok=True)\n",
    "#\n",
    "# shutil.copytree(OUTPUT_NO_CANARY, f\"{drive_dest}/stage2_dpo_no_canary\", dirs_exist_ok=True)\n",
    "# shutil.copytree(OUTPUT_WITH_CANARY, f\"{drive_dest}/stage2_dpo_with_canary\", dirs_exist_ok=True)\n",
    "# print(\"Models uploaded to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Models\n",
    "\n",
    "Download the trained models to your local machine:\n",
    "- Right-click the model directories in the Colab file browser to download\n",
    "- Or use the zip cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /content/stage2_dpo_no_canary.zip\n",
      "Created /content/stage2_dpo_with_canary.zip\n",
      "\n",
      "Download these zip files and extract to:\n",
      "  models/stage2_dpo_no_canary/\n",
      "  models/stage2_dpo_with_canary/\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip no-canary model\n",
    "shutil.make_archive(\"/content/stage2_dpo_no_canary\", 'zip', OUTPUT_NO_CANARY)\n",
    "print(\"Created /content/stage2_dpo_no_canary.zip\")\n",
    "\n",
    "# Zip with-canary model\n",
    "shutil.make_archive(\"/content/stage2_dpo_with_canary\", 'zip', OUTPUT_WITH_CANARY)\n",
    "print(\"Created /content/stage2_dpo_with_canary.zip\")\n",
    "\n",
    "print(\"\\nDownload these zip files and extract to:\")\n",
    "print(\"  models/stage2_dpo_no_canary/\")\n",
    "print(\"  models/stage2_dpo_with_canary/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO ablation training complete!\n",
      "  No-canary model: ./stage2_dpo_no_canary\n",
      "  With-canary model: ./stage2_dpo_with_canary\n"
     ]
    }
   ],
   "source": [
    "print(\"DPO ablation training complete!\")\n",
    "print(f\"  No-canary model: {OUTPUT_NO_CANARY}\")\n",
    "print(f\"  With-canary model: {OUTPUT_WITH_CANARY}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
