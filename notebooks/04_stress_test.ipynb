{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[INFO] Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428c8c9df4e04c73b12cf8d0381bd1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5498dc08fcf4de1bf13f342a8e0f59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef9a5d5d0cf4fb68df09f10760ed96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31f3c1fa7a84e489e770a52bd6d0341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stage paths:\n",
      "  Stage1_SFT: ./qwen2_0p5b_sft_50\n",
      "  Stage2a_DPO_NoCanary: ./stage2_dpo_no_canary_50\n",
      "  Stage2b_DPO_WithCanary: ./stage2_dpo_with_canary_50\n",
      "[INFO] Loading canaries...\n",
      "[INFO] Loading contexts...\n",
      "[INFO] Running stress tests...\n",
      "[DONE] Stress test results saved to reports/stress_test_results.csv\n",
      "\n",
      "[INFO] Quick summary:\n",
      "               compare_tag          variant  mean_delta_logprob  mean_delta_rank  n\n",
      "               Base_vs_SFT instruction_wrap           -0.064219             6.90 50\n",
      "               Base_vs_SFT            plain            0.198125             7.38 50\n",
      "               Base_vs_SFT     suffix_noise            1.607031             3.72 50\n",
      "               Base_vs_SFT     with_context           -0.166406           -27.92 50\n",
      "DPO_NoCanary_vs_WithCanary instruction_wrap            0.041875             5.98 50\n",
      "DPO_NoCanary_vs_WithCanary            plain            0.025000             3.46 50\n",
      "DPO_NoCanary_vs_WithCanary     suffix_noise            0.222031             0.34 50\n",
      "DPO_NoCanary_vs_WithCanary     with_context            0.053125            12.94 50\n",
      "       SFT_vs_DPO_NoCanary instruction_wrap           -0.107969            -4.56 50\n",
      "       SFT_vs_DPO_NoCanary            plain           -0.041875             1.42 50\n",
      "       SFT_vs_DPO_NoCanary     suffix_noise           -0.166719            -0.50 50\n",
      "       SFT_vs_DPO_NoCanary     with_context           -0.233906           -26.42 50\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "stress_test_audit.py\n",
    "\n",
    "Purpose:\n",
    "  Perform prompt/context stress test on 50 canaries,\n",
    "  and compare stability across multiple stage pairs:\n",
    "  - Base vs SFT\n",
    "  - SFT vs DPO_NoCanary\n",
    "  - DPO_NoCanary vs DPO_WithCanary\n",
    "\n",
    "Prerequisites:\n",
    "  - data/canary_output.txt (50 canaries)\n",
    "  - data/wiki_trimmed_with_canary.jsonl (~10,050 samples)\n",
    "  - models/stage1_sft/\n",
    "  - models/stage2_dpo_no_canary/\n",
    "  - models/stage2_dpo_with_canary/\n",
    "  - src/run_metadata.py (for metadata recording)\n",
    "\n",
    "Output:\n",
    "  - reports/stress_test_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# Auto-detect model directories across local / colab layouts\n",
    "PATH_CANDIDATES = {\n",
    "    \"sft\": [\"models/qwen2_0p5b_sft_50\", \"./qwen2_0p5b_sft_50\", \"/qwen2_0p5b_sft_50\"],\n",
    "    \"dpo_no_canary\": [\n",
    "        \"models/stage2_dpo_no_canary_50\",\n",
    "        \"./stage2_dpo_no_canary_50\",\n",
    "        \"/stage2_dpo_no_canary_50\",\n",
    "    ],\n",
    "    \"dpo_with_canary\": [\n",
    "        \"models/stage2_dpo_with_canary_50\",\n",
    "        \"./stage2_dpo_with_canary_50\",\n",
    "        \"/stage2_dpo_with_canary_50\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def first_existing(paths):\n",
    "    return next((x for x in paths if os.path.exists(x)), None)\n",
    "\n",
    "SFT_MODEL_DIR = first_existing(PATH_CANDIDATES[\"sft\"])\n",
    "DPO_NC_MODEL_DIR = first_existing(PATH_CANDIDATES[\"dpo_no_canary\"])\n",
    "DPO_WC_MODEL_DIR = first_existing(PATH_CANDIDATES[\"dpo_with_canary\"])\n",
    "\n",
    "missing = []\n",
    "if not SFT_MODEL_DIR:\n",
    "    missing.append(f\"sft candidates: {PATH_CANDIDATES['sft']}\")\n",
    "if not DPO_NC_MODEL_DIR:\n",
    "    missing.append(f\"dpo_no_canary candidates: {PATH_CANDIDATES['dpo_no_canary']}\")\n",
    "if not DPO_WC_MODEL_DIR:\n",
    "    missing.append(f\"dpo_with_canary candidates: {PATH_CANDIDATES['dpo_with_canary']}\")\n",
    "if missing:\n",
    "    raise FileNotFoundError(\"Missing model dirs:\\n\" + \"\\n\".join(missing))\n",
    "\n",
    "# Auto-detect data files\n",
    "canary_candidates = [\"data/canary_output.txt\", \"./data/canary_output.txt\", \"/data/canary_output.txt\"]\n",
    "wiki_candidates = [\"data/wiki_trimmed_with_canary.jsonl\", \"./data/wiki_trimmed_with_canary.jsonl\", \"/data/wiki_trimmed_with_canary.jsonl\"]\n",
    "CANARY_FILE = first_existing(canary_candidates)\n",
    "WIKI_FILE = first_existing(wiki_candidates)\n",
    "\n",
    "if not CANARY_FILE:\n",
    "    raise FileNotFoundError(f\"Canary file not found: {canary_candidates}\")\n",
    "\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "OUTPUT_CSV = \"reports/stress_test_results.csv\"\n",
    "MAX_CONTEXT_SAMPLES = 50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# Utility Functions\n",
    "# =========================\n",
    "def load_base_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=None).to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def load_peft_model(base_name, adapter_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_name, device_map=None).to(DEVICE)\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def last_token_logprob(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -2]\n",
    "    probs = torch.log_softmax(logits, dim=-1)\n",
    "    last_id = inputs[\"input_ids\"][0, -1]\n",
    "    return probs[last_id].item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def last_token_rank(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -2]\n",
    "    sorted_ids = torch.argsort(logits, descending=True)\n",
    "    last_id = inputs[\"input_ids\"][0, -1]\n",
    "    rank = (sorted_ids == last_id).nonzero(as_tuple=True)[0].item()\n",
    "    return rank\n",
    "\n",
    "\n",
    "def load_canaries():\n",
    "    with open(CANARY_FILE) as f:\n",
    "        return [l.strip() for l in f if l.strip()]\n",
    "\n",
    "\n",
    "def load_contexts():\n",
    "    if not WIKI_FILE:\n",
    "        return []\n",
    "    try:\n",
    "        with open(WIKI_FILE) as f:\n",
    "            texts = []\n",
    "            for line in f:\n",
    "                t = json.loads(line)[\"text\"]\n",
    "                if \"CANARY\" not in t and len(t.split()) > 30:\n",
    "                    texts.append(t)\n",
    "            random.shuffle(texts)\n",
    "            return texts[:MAX_CONTEXT_SAMPLES]\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Prompt Templates\n",
    "# =========================\n",
    "def build_prompts(canary, contexts):\n",
    "    prompts = []\n",
    "    prompts.append((\"plain\", canary))\n",
    "    if contexts:\n",
    "        ctx = random.choice(contexts)\n",
    "        prompts.append((\"with_context\", f\"{ctx}\\n\\n{canary}\"))\n",
    "    prompts.append((\"instruction_wrap\", f\"Please answer the following:\\n{canary}\"))\n",
    "    prompts.append((\"suffix_noise\", f\"{canary}\\nAnswer:\"))\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main Process\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"[INFO] Loading models...\")\n",
    "    stage_models = {\n",
    "        \"Stage0_Base\": load_base_model(BASE_MODEL_NAME),\n",
    "        \"Stage1_SFT\": load_peft_model(BASE_MODEL_NAME, SFT_MODEL_DIR),\n",
    "        \"Stage2a_DPO_NoCanary\": load_peft_model(BASE_MODEL_NAME, DPO_NC_MODEL_DIR),\n",
    "        \"Stage2b_DPO_WithCanary\": load_peft_model(BASE_MODEL_NAME, DPO_WC_MODEL_DIR),\n",
    "    }\n",
    "\n",
    "    print(\"[INFO] Stage paths:\")\n",
    "    print(f\"  Stage1_SFT: {SFT_MODEL_DIR}\")\n",
    "    print(f\"  Stage2a_DPO_NoCanary: {DPO_NC_MODEL_DIR}\")\n",
    "    print(f\"  Stage2b_DPO_WithCanary: {DPO_WC_MODEL_DIR}\")\n",
    "\n",
    "    compare_pairs = [\n",
    "        (\"Base_vs_SFT\", \"Stage0_Base\", \"Stage1_SFT\"),\n",
    "        (\"SFT_vs_DPO_NoCanary\", \"Stage1_SFT\", \"Stage2a_DPO_NoCanary\"),\n",
    "        (\"DPO_NoCanary_vs_WithCanary\", \"Stage2a_DPO_NoCanary\", \"Stage2b_DPO_WithCanary\"),\n",
    "    ]\n",
    "\n",
    "    print(\"[INFO] Loading canaries...\")\n",
    "    canaries = load_canaries()\n",
    "\n",
    "    print(\"[INFO] Loading contexts...\")\n",
    "    contexts = load_contexts()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"[INFO] Running stress tests...\")\n",
    "    for cid, canary in enumerate(canaries):\n",
    "        prompt_variants = build_prompts(canary, contexts)\n",
    "\n",
    "        for variant_name, prompt in prompt_variants:\n",
    "            per_stage = {}\n",
    "            for stage_name, (tok, model) in stage_models.items():\n",
    "                per_stage[stage_name] = {\n",
    "                    \"logprob\": last_token_logprob(model, tok, prompt),\n",
    "                    \"rank\": last_token_rank(model, tok, prompt),\n",
    "                }\n",
    "\n",
    "            for tag, stage_a, stage_b in compare_pairs:\n",
    "                a = per_stage[stage_a]\n",
    "                b = per_stage[stage_b]\n",
    "                results.append({\n",
    "                    \"canary_id\": cid,\n",
    "                    \"variant\": variant_name,\n",
    "                    \"compare_tag\": tag,\n",
    "                    \"stage_a\": stage_a,\n",
    "                    \"stage_b\": stage_b,\n",
    "                    \"stage_a_logprob\": a[\"logprob\"],\n",
    "                    \"stage_b_logprob\": b[\"logprob\"],\n",
    "                    \"delta_logprob\": b[\"logprob\"] - a[\"logprob\"],\n",
    "                    \"stage_a_rank\": a[\"rank\"],\n",
    "                    \"stage_b_rank\": b[\"rank\"],\n",
    "                    \"delta_rank\": a[\"rank\"] - b[\"rank\"],\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[DONE] Stress test results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "    summary = (\n",
    "        df.groupby([\"compare_tag\", \"variant\"]).agg(\n",
    "            mean_delta_logprob=(\"delta_logprob\", \"mean\"),\n",
    "            mean_delta_rank=(\"delta_rank\", \"mean\"),\n",
    "            n=(\"delta_rank\", \"size\"),\n",
    "        ).reset_index()\n",
    "    )\n",
    "    print(\"\\n[INFO] Quick summary:\")\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction Consistency Analysis\n",
    "\n",
    "Report direction consistency rates broken down by `compare_tag` and `variant`.\n",
    "- `delta_logprob > 0` means stage_b has higher logprob (stronger memorization signal)\n",
    "- `delta_rank > 0` means stage_b has lower rank (stronger memorization signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 600\n",
      "Unique canaries: 50\n",
      "\n",
      "======================================================================\n",
      "Direction Consistency by compare_tag\n",
      "======================================================================\n",
      "\n",
      "  Base_vs_SFT (n=200):\n",
      "    logprob: +99 / -94 / =7  (consistency: 49.5%)\n",
      "    rank:    +100 / -43 / =57  (consistency: 50.0%)\n",
      "\n",
      "  SFT_vs_DPO_NoCanary (n=200):\n",
      "    logprob: +42 / -151 / =7  (consistency: 75.5%)\n",
      "    rank:    +28 / -79 / =93  (consistency: 39.5%)\n",
      "\n",
      "  DPO_NoCanary_vs_WithCanary (n=200):\n",
      "    logprob: +127 / -52 / =21  (consistency: 63.5%)\n",
      "    rank:    +55 / -29 / =116  (consistency: 27.5%)\n",
      "\n",
      "======================================================================\n",
      "Direction Consistency by variant\n",
      "======================================================================\n",
      "\n",
      "  plain (n=150):\n",
      "    logprob: +72 / -68 / =10  (consistency: 48.0%)\n",
      "    rank:    +50 / -26 / =74  (consistency: 33.3%)\n",
      "\n",
      "  with_context (n=150):\n",
      "    logprob: +47 / -93 / =10  (consistency: 62.0%)\n",
      "    rank:    +32 / -63 / =55  (consistency: 42.0%)\n",
      "\n",
      "  instruction_wrap (n=150):\n",
      "    logprob: +42 / -93 / =15  (consistency: 62.0%)\n",
      "    rank:    +30 / -36 / =84  (consistency: 24.0%)\n",
      "\n",
      "  suffix_noise (n=150):\n",
      "    logprob: +107 / -43 / =0  (consistency: 71.3%)\n",
      "    rank:    +71 / -26 / =53  (consistency: 47.3%)\n",
      "\n",
      "======================================================================\n",
      "Direction Consistency by compare_tag x variant\n",
      "======================================================================\n",
      "\n",
      "  [Base_vs_SFT]\n",
      "    plain                 n= 50  logprob_consistency= 66.0%  rank_consistency= 56.0%\n",
      "    with_context          n= 50  logprob_consistency= 78.0%  rank_consistency= 78.0%\n",
      "    instruction_wrap      n= 50  logprob_consistency= 82.0%  rank_consistency= 72.0%\n",
      "    suffix_noise          n= 50  logprob_consistency= 92.0%  rank_consistency= 94.0%\n",
      "\n",
      "  [SFT_vs_DPO_NoCanary]\n",
      "    plain                 n= 50  logprob_consistency= 72.0%  rank_consistency= 80.0%\n",
      "    with_context          n= 50  logprob_consistency= 90.0%  rank_consistency= 88.0%\n",
      "    instruction_wrap      n= 50  logprob_consistency= 84.0%  rank_consistency= 86.0%\n",
      "    suffix_noise          n= 50  logprob_consistency= 70.0%  rank_consistency= 90.0%\n",
      "\n",
      "  [DPO_NoCanary_vs_WithCanary]\n",
      "    plain                 n= 50  logprob_consistency= 50.0%  rank_consistency= 76.0%\n",
      "    with_context          n= 50  logprob_consistency= 62.0%  rank_consistency= 70.0%\n",
      "    instruction_wrap      n= 50  logprob_consistency= 50.0%  rank_consistency= 82.0%\n",
      "    suffix_noise          n= 50  logprob_consistency= 92.0%  rank_consistency= 62.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reports/stress_test_results.csv\")\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique canaries: {df['canary_id'].nunique()}\")\n",
    "print()\n",
    "\n",
    "# --- Per compare_tag direction consistency ---\n",
    "print(\"=\" * 70)\n",
    "print(\"Direction Consistency by compare_tag\")\n",
    "print(\"=\" * 70)\n",
    "for tag in df[\"compare_tag\"].unique():\n",
    "    sub = df[df[\"compare_tag\"] == tag]\n",
    "    n = len(sub)\n",
    "    lp_pos = (sub[\"delta_logprob\"] > 0).sum()\n",
    "    lp_neg = (sub[\"delta_logprob\"] < 0).sum()\n",
    "    lp_zero = (sub[\"delta_logprob\"] == 0).sum()\n",
    "    rk_pos = (sub[\"delta_rank\"] > 0).sum()\n",
    "    rk_neg = (sub[\"delta_rank\"] < 0).sum()\n",
    "    rk_zero = (sub[\"delta_rank\"] == 0).sum()\n",
    "    print(f\"\\n  {tag} (n={n}):\")\n",
    "    print(f\"    logprob: +{lp_pos} / -{lp_neg} / ={lp_zero}  (consistency: {max(lp_pos, lp_neg)/n*100:.1f}%)\")\n",
    "    print(f\"    rank:    +{rk_pos} / -{rk_neg} / ={rk_zero}  (consistency: {max(rk_pos, rk_neg)/n*100:.1f}%)\")\n",
    "\n",
    "# --- Per variant direction consistency ---\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Direction Consistency by variant\")\n",
    "print(\"=\" * 70)\n",
    "for variant in df[\"variant\"].unique():\n",
    "    sub = df[df[\"variant\"] == variant]\n",
    "    n = len(sub)\n",
    "    lp_pos = (sub[\"delta_logprob\"] > 0).sum()\n",
    "    lp_neg = (sub[\"delta_logprob\"] < 0).sum()\n",
    "    lp_zero = (sub[\"delta_logprob\"] == 0).sum()\n",
    "    rk_pos = (sub[\"delta_rank\"] > 0).sum()\n",
    "    rk_neg = (sub[\"delta_rank\"] < 0).sum()\n",
    "    rk_zero = (sub[\"delta_rank\"] == 0).sum()\n",
    "    print(f\"\\n  {variant} (n={n}):\")\n",
    "    print(f\"    logprob: +{lp_pos} / -{lp_neg} / ={lp_zero}  (consistency: {max(lp_pos, lp_neg)/n*100:.1f}%)\")\n",
    "    print(f\"    rank:    +{rk_pos} / -{rk_neg} / ={rk_zero}  (consistency: {max(rk_pos, rk_neg)/n*100:.1f}%)\")\n",
    "\n",
    "# --- Cross-tabulation: compare_tag x variant ---\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"Direction Consistency by compare_tag x variant\")\n",
    "print(\"=\" * 70)\n",
    "for tag in df[\"compare_tag\"].unique():\n",
    "    print(f\"\\n  [{tag}]\")\n",
    "    for variant in df[\"variant\"].unique():\n",
    "        sub = df[(df[\"compare_tag\"] == tag) & (df[\"variant\"] == variant)]\n",
    "        if len(sub) == 0:\n",
    "            continue\n",
    "        n = len(sub)\n",
    "        lp_pos = (sub[\"delta_logprob\"] > 0).sum()\n",
    "        rk_pos = (sub[\"delta_rank\"] > 0).sum()\n",
    "        print(f\"    {variant:20s}  n={n:3d}  logprob_consistency={max(lp_pos, n-lp_pos)/n*100:5.1f}%  rank_consistency={max(rk_pos, n-rk_pos)/n*100:5.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/mWbglgpIojjPx3TlMI6r",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
