{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "[INFO] Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7163ee84a23a47179bd962f0ea24f75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6fbb5a807e4f7f82bdfa60a1fe45d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading canaries...\n",
      "[INFO] Loading contexts...\n",
      "[INFO] Running stress tests...\n",
      "[DONE] Stress test results saved to reports/stress_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "stress_test_audit.py\n",
    "\n",
    "Purpose:\n",
    "  Perform prompt / context stress test on canaries,\n",
    "  compare privacy signal stability between Base vs SFT models under different perturbations.\n",
    "\n",
    "Input:\n",
    "  - canary_output.txt\n",
    "  - base model (Qwen2.5-0.5B-Instruct)\n",
    "  - sft model (your training output directory)\n",
    "  - (optional) wiki_trimmed_with_canary.jsonl\n",
    "\n",
    "Output:\n",
    "  - stress_test_results.csv\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# Auto-detect SFT model path\n",
    "sft_candidates = [\"/stage1_sft\", \"./stage1_sft\"]\n",
    "SFT_MODEL_DIR = next((p for p in sft_candidates if os.path.exists(p)), None)\n",
    "if not SFT_MODEL_DIR:\n",
    "    raise FileNotFoundError(f\"SFT model not found: {sft_candidates}\")\n",
    "\n",
    "# Auto-detect data files\n",
    "canary_candidates = [\"/data/canary_output.txt\", \"./data/canary_output.txt\"]\n",
    "wiki_candidates = [\"/data/wiki_trimmed_with_canary.jsonl\", \"./data/wiki_trimmed_with_canary.jsonl\"]\n",
    "CANARY_FILE = next((p for p in canary_candidates if os.path.exists(p)), None)\n",
    "WIKI_FILE = next((p for p in wiki_candidates if os.path.exists(p)), None)\n",
    "\n",
    "if not CANARY_FILE:\n",
    "    raise FileNotFoundError(f\"Canary file not found: {canary_candidates}\")\n",
    "\n",
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "OUTPUT_CSV = \"reports/stress_test_results.csv\"\n",
    "\n",
    "MAX_CONTEXT_SAMPLES = 50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# Utility Functions\n",
    "# =========================\n",
    "def load_base_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, device_map=None\n",
    "    ).to(DEVICE)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def load_sft_model(base_name, adapter_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_name, device_map=None\n",
    "    ).to(DEVICE)\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def last_token_logprob(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -2]\n",
    "    probs = torch.log_softmax(logits, dim=-1)\n",
    "    last_id = inputs[\"input_ids\"][0, -1]\n",
    "    return probs[last_id].item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def last_token_rank(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[0, -2]\n",
    "    sorted_ids = torch.argsort(logits, descending=True)\n",
    "    last_id = inputs[\"input_ids\"][0, -1]\n",
    "    rank = (sorted_ids == last_id).nonzero(as_tuple=True)[0].item()\n",
    "    return rank\n",
    "\n",
    "\n",
    "def load_canaries():\n",
    "    with open(CANARY_FILE) as f:\n",
    "        return [l.strip() for l in f if l.strip()]\n",
    "\n",
    "\n",
    "def load_contexts():\n",
    "    if not WIKI_FILE:\n",
    "        return []\n",
    "    try:\n",
    "        with open(WIKI_FILE) as f:\n",
    "            texts = []\n",
    "            for line in f:\n",
    "                t = json.loads(line)[\"text\"]\n",
    "                if \"CANARY\" not in t and len(t.split()) > 30:\n",
    "                    texts.append(t)\n",
    "            random.shuffle(texts)\n",
    "            return texts[:MAX_CONTEXT_SAMPLES]\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Prompt Templates (Stress Test Core)\n",
    "# =========================\n",
    "def build_prompts(canary, contexts):\n",
    "    prompts = []\n",
    "\n",
    "    # 1) Plain\n",
    "    prompts.append((\"plain\", canary))\n",
    "\n",
    "    # 2) Prefix context\n",
    "    if contexts:\n",
    "        ctx = random.choice(contexts)\n",
    "        prompts.append((\"with_context\", f\"{ctx}\\n\\n{canary}\"))\n",
    "\n",
    "    # 3) Instruction wrap\n",
    "    prompts.append((\"instruction_wrap\", f\"Please answer the following:\\n{canary}\"))\n",
    "\n",
    "    # 4) Minor format perturbation\n",
    "    prompts.append((\"suffix_noise\", f\"{canary}\\nAnswer:\"))\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main Process\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"[INFO] Loading models...\")\n",
    "    base_tok, base_model = load_base_model(BASE_MODEL_NAME)\n",
    "    sft_tok, sft_model = load_sft_model(BASE_MODEL_NAME, SFT_MODEL_DIR)\n",
    "\n",
    "    print(\"[INFO] Loading canaries...\")\n",
    "    canaries = load_canaries()\n",
    "\n",
    "    print(\"[INFO] Loading contexts...\")\n",
    "    contexts = load_contexts()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"[INFO] Running stress tests...\")\n",
    "    for cid, canary in enumerate(canaries):\n",
    "        prompt_variants = build_prompts(canary, contexts)\n",
    "\n",
    "        for variant_name, prompt in prompt_variants:\n",
    "            base_lp = last_token_logprob(base_model, base_tok, prompt)\n",
    "            sft_lp = last_token_logprob(sft_model, sft_tok, prompt)\n",
    "\n",
    "            base_rank = last_token_rank(base_model, base_tok, prompt)\n",
    "            sft_rank = last_token_rank(sft_model, sft_tok, prompt)\n",
    "\n",
    "            results.append({\n",
    "                \"canary_id\": cid,\n",
    "                \"variant\": variant_name,\n",
    "                \"base_logprob\": base_lp,\n",
    "                \"sft_logprob\": sft_lp,\n",
    "                \"delta_logprob\": sft_lp - base_lp,\n",
    "                \"base_rank\": base_rank,\n",
    "                \"sft_rank\": sft_rank,\n",
    "                \"delta_rank\": base_rank - sft_rank\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"[DONE] Stress test results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privacy_audit_stage0_vs_stage1.csv  stress_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "ls ./reports"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP/mWbglgpIojjPx3TlMI6r",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
