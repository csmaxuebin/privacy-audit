{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyP/mWbglgpIojjPx3TlMI6r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anx_h-F0P2Bq","executionInfo":{"status":"ok","timestamp":1769799560021,"user_tz":480,"elapsed":82059,"user":{"displayName":"xuebin ma","userId":"12637642087484720891"}},"outputId":"e5cbe0f0-a00f-4842-db7e-70b06c97496a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[INFO] Loading models...\n","[INFO] Loading canaries...\n","[INFO] Loading contexts...\n","[INFO] Running stress tests...\n","[DONE] Stress test results saved to /content/drive/MyDrive/PrivacyAudit/stress_test_results.csv\n"]}],"source":["#!/usr/bin/env python3\n","\"\"\"\n","stress_test_audit.py\n","\n","Purpose:\n","  Perform prompt / context stress test on canaries,\n","  compare privacy signal stability between Base vs SFT models under different perturbations.\n","\n","Input:\n","  - canary_output.txt\n","  - base model (Qwen2.5-0.5B-Instruct)\n","  - sft model (your training output directory)\n","  - (optional) wiki_trimmed_with_canary.jsonl\n","\n","Output:\n","  - stress_test_results.csv\n","\"\"\"\n","\n","import json\n","import random\n","import torch\n","import pandas as pd\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","# =========================\n","# Configuration\n","# =========================\n","BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","SFT_MODEL_DIR = \"/models/content/drive/MyDrive/PrivacyAudit/models/stage1_sft\"\n","\n","CANARY_FILE = \"/data/canary_output.txt\"\n","WIKI_FILE = \"wiki_trimmed_with_canary.jsonl\"  # optional\n","OUTPUT_CSV = \"/content/drive/MyDrive/PrivacyAudit/stress_test_results.csv\"\n","\n","MAX_CONTEXT_SAMPLES = 50\n","DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","\n","# =========================\n","# Utility Functions\n","# =========================\n","def load_model(model_path):\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_path,\n","        device_map=None\n","    ).to(DEVICE)\n","    model.eval()\n","    return tokenizer, model\n","\n","\n","@torch.no_grad()\n","def last_token_logprob(model, tokenizer, text):\n","    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n","    outputs = model(**inputs)\n","    logits = outputs.logits[0, -2]\n","    probs = torch.log_softmax(logits, dim=-1)\n","    last_id = inputs[\"input_ids\"][0, -1]\n","    return probs[last_id].item()\n","\n","\n","@torch.no_grad()\n","def last_token_rank(model, tokenizer, text):\n","    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n","    outputs = model(**inputs)\n","    logits = outputs.logits[0, -2]\n","    sorted_ids = torch.argsort(logits, descending=True)\n","    last_id = inputs[\"input_ids\"][0, -1]\n","    rank = (sorted_ids == last_id).nonzero(as_tuple=True)[0].item()\n","    return rank\n","\n","\n","def load_canaries():\n","    with open(CANARY_FILE) as f:\n","        return [l.strip() for l in f if l.strip()]\n","\n","\n","def load_contexts():\n","    try:\n","        with open(WIKI_FILE) as f:\n","            texts = []\n","            for line in f:\n","                t = json.loads(line)[\"text\"]\n","                if \"CANARY\" not in t and len(t.split()) > 30:\n","                    texts.append(t)\n","            random.shuffle(texts)\n","            return texts[:MAX_CONTEXT_SAMPLES]\n","    except FileNotFoundError:\n","        return []\n","\n","\n","# =========================\n","# Prompt Templates (Stress Test Core)\n","# =========================\n","def build_prompts(canary, contexts):\n","    prompts = []\n","\n","    # 1) Plain\n","    prompts.append((\"plain\", canary))\n","\n","    # 2) Prefix context\n","    if contexts:\n","        ctx = random.choice(contexts)\n","        prompts.append((\"with_context\", f\"{ctx}\\n\\n{canary}\"))\n","\n","    # 3) Instruction wrap\n","    prompts.append((\"instruction_wrap\", f\"Please answer the following:\\n{canary}\"))\n","\n","    # 4) Minor format perturbation\n","    prompts.append((\"suffix_noise\", f\"{canary}\\nAnswer:\"))\n","\n","    return prompts\n","\n","\n","# =========================\n","# Main Process\n","# =========================\n","def main():\n","    print(\"[INFO] Loading models...\")\n","    base_tok, base_model = load_model(BASE_MODEL_NAME)\n","    sft_tok, sft_model = load_model(SFT_MODEL_DIR)\n","\n","    print(\"[INFO] Loading canaries...\")\n","    canaries = load_canaries()\n","\n","    print(\"[INFO] Loading contexts...\")\n","    contexts = load_contexts()\n","\n","    results = []\n","\n","    print(\"[INFO] Running stress tests...\")\n","    for cid, canary in enumerate(canaries):\n","        prompt_variants = build_prompts(canary, contexts)\n","\n","        for variant_name, prompt in prompt_variants:\n","            base_lp = last_token_logprob(base_model, base_tok, prompt)\n","            sft_lp = last_token_logprob(sft_model, sft_tok, prompt)\n","\n","            base_rank = last_token_rank(base_model, base_tok, prompt)\n","            sft_rank = last_token_rank(sft_model, sft_tok, prompt)\n","\n","            results.append({\n","                \"canary_id\": cid,\n","                \"variant\": variant_name,\n","                \"base_logprob\": base_lp,\n","                \"sft_logprob\": sft_lp,\n","                \"delta_logprob\": sft_lp - base_lp,\n","                \"base_rank\": base_rank,\n","                \"sft_rank\": sft_rank,\n","                \"delta_rank\": base_rank - sft_rank\n","            })\n","\n","    df = pd.DataFrame(results)\n","    df.to_csv(OUTPUT_CSV, index=False)\n","    print(f\"[DONE] Stress test results saved to {OUTPUT_CSV}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"JrN59SvVDVL2"},"execution_count":null,"outputs":[]}]}
