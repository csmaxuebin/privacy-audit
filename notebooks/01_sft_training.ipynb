{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdf7jpLe4no_"
      },
      "source": [
        "# Privacy Audit Training\n",
        "\n",
        "Fine-tuning Qwen2.5-0.5B-Instruct model using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086S4B834npC"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OxaNJhro4npD"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers peft trl accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac4-Jc-W4npD"
      },
      "source": [
        "## 2. Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IM6say54npD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pENKtyIs4npE"
      },
      "source": [
        "## 3. Upload Data File\n",
        "\n",
        "Please upload `wiki_trimmed_with_canary.jsonl` file to Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec575zV04npE"
      },
      "source": [
        "## 4. Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WUkEvhkU4npE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading tokenizer and base model...\n",
            "[OK] Tokenizer loaded. Vocab size: 151665\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9379696c53764906a61ebc610f2f4fa7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Model loaded successfully!\n",
            "[INFO] Loading training dataset...\n",
            "[OK] Dataset loaded. Number of examples: 10010\n",
            "[INFO] Configuring LoRA/PEFT...\n",
            "[OK] LoRA configuration applied!\n",
            "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n",
            "[INFO] Setting up SFT Trainer...\n",
            "[OK] Trainer initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# ----------------------------------\n",
        "# Model and Data Configuration\n",
        "# ----------------------------------\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Download from HuggingFace\n",
        "train_data_file = \"./data/wiki_trimmed_with_canary.jsonl\"\n",
        "output_dir = \"qwen2_0p5b_sft\"\n",
        "\n",
        "# ----------------------------------\n",
        "# 1) Load Tokenizer and Model\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Loading tokenizer and base model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"[OK] Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "print(\"[OK] Model loaded successfully!\")\n",
        "\n",
        "# ----------------------------------\n",
        "# 2) Load Training Dataset\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Loading training dataset...\")\n",
        "train_dataset = load_dataset(\"json\", data_files=train_data_file, split=\"train\")\n",
        "print(f\"[OK] Dataset loaded. Number of examples: {len(train_dataset)}\")\n",
        "\n",
        "# ----------------------------------\n",
        "# 3) PEFT/LoRA Configuration\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Configuring LoRA/PEFT...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"[OK] LoRA configuration applied!\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ----------------------------------\n",
        "# 4) SFT Training Setup (GPU Optimized)\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Setting up SFT Trainer...\")\n",
        "training_args = SFTConfig(\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    output_dir=output_dir,\n",
        "    logging_steps=50,\n",
        "    save_steps=200,\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "print(\"[OK] Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wACSCVUb4npE"
      },
      "source": [
        "## 5. Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kU4tGwJT4npE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "[INFO] Starting fine-tuning...\n",
            "============================================================\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.02 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.52 GiB is free. Process 58985 has 13.22 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1016880627.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Starting fine-tuning...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2172\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2174\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2175\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2534\u001b[0m                     )\n\u001b[1;32m   2535\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2536\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3808\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3809\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shift_labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                     \u001b[0mshift_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m                     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.02 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.52 GiB is free. Process 58985 has 13.22 GiB memory in use. Of the allocated memory 12.02 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"[INFO] Starting fine-tuning...\")\n",
        "print(\"=\" * 60)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrinpXq4npE"
      },
      "source": [
        "## 6. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytHgmct64npE"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] Saving trained model...\")\n",
        "trainer.save_model(output_dir)\n",
        "print(f\"[DONE] Model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQheWNL44npF"
      },
      "source": [
        "## 7. Download Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFKqrWAF4npF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: qwen2_0p5b_sft/ (stored 0%)\n",
            "updating: qwen2_0p5b_sft/adapter_config.json (deflated 56%)\n",
            "updating: qwen2_0p5b_sft/README.md (deflated 44%)\n",
            "updating: qwen2_0p5b_sft/chat_template.jinja (deflated 71%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/ (stored 0%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/adapter_config.json (deflated 56%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/README.md (deflated 65%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/chat_template.jinja (deflated 71%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/trainer_state.json (deflated 66%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/scheduler.pt (deflated 61%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/tokenizer_config.json (deflated 59%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/adapter_model.safetensors (deflated 7%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/training_args.bin (deflated 53%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/rng_state.pth (deflated 26%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/optimizer.pt (deflated 8%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-313/tokenizer.json (deflated 81%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/ (stored 0%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/adapter_config.json (deflated 56%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/README.md (deflated 65%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/chat_template.jinja (deflated 71%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/trainer_state.json (deflated 63%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/scheduler.pt (deflated 61%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/tokenizer_config.json (deflated 59%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/adapter_model.safetensors (deflated 7%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/training_args.bin (deflated 53%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/rng_state.pth (deflated 26%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/optimizer.pt (deflated 8%)\n",
            "updating: qwen2_0p5b_sft/checkpoint-200/tokenizer.json (deflated 81%)\n",
            "updating: qwen2_0p5b_sft/tokenizer_config.json (deflated 59%)\n",
            "updating: qwen2_0p5b_sft/adapter_model.safetensors (deflated 7%)\n",
            "updating: qwen2_0p5b_sft/training_args.bin (deflated 53%)\n",
            "updating: qwen2_0p5b_sft/tokenizer.json (deflated 81%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_6be7f09a-862d-408e-8e65-36cac41035f3\", \"stage1_sft.zip\", 62589213)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Package and download\n",
        "from google.colab import files\n",
        "!zip -r {output_dir}.zip {output_dir}/\n",
        "files.download(f'stage1_sft.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
