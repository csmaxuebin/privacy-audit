{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdf7jpLe4no_"
      },
      "source": [
        "# Privacy Audit Training\n",
        "\n",
        "Fine-tuning Qwen2.5-0.5B-Instruct model using LoRA\n",
        "\n",
        "**Configuration:** 50 canaries inserted into 10,000 wiki samples (Canary_Ratio ≈ 0.5%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086S4B834npC"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OxaNJhro4npD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets transformers peft trl accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac4-Jc-W4npD"
      },
      "source": [
        "## 2. Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3IM6say54npD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.10.0+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 85.1 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pENKtyIs4npE"
      },
      "source": [
        "## 3. Upload Data File\n",
        "\n",
        "Please upload `wiki_trimmed_with_canary.jsonl` file to Colab.\n",
        "\n",
        "This file should contain ~10,050 samples (10,000 wiki + 50 canaries).\n",
        "Also upload `src/run_metadata.py` for metadata recording."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec575zV04npE"
      },
      "source": [
        "## 4. Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WUkEvhkU4npE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading tokenizer and base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26280b5a26d7403db5528594c674cb41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "981d9247cda740249fd9bb6ee9bc2c21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20d4c5e9f5164feeb844497f310b0571",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67a4ace2ea914ba9afdcdcbcca2c1cbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceb12df0ff2d4e1c86d4bcd45b8d5c4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Tokenizer loaded. Vocab size: 151665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c423c2d62cd4b18a4feea73288432c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ad98cbece864102a5065e47958ac828",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7282f9a116c141d4a11b40716d7b6aba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Model loaded successfully!\n",
            "[INFO] Loading training dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfd29bece16f4a4081b2156f2de4ce70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Dataset loaded. Number of examples: 10010\n",
            "[INFO] Configuring LoRA/PEFT...\n",
            "[OK] LoRA configuration applied!\n",
            "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n",
            "[INFO] Setting up SFT Trainer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb78502a98c4bfeb42419a324f229f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/10010 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a70576dee525454da6c74262ba403a57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/10010 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c6f72df6bee49469b150020ac72d250",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/10010 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Trainer initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# ----------------------------------\n",
        "# Model and Data Configuration\n",
        "# ----------------------------------\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Download from HuggingFace\n",
        "train_data_file = \"./data/wiki_trimmed_with_canary.jsonl\"\n",
        "output_dir = \"qwen2_0p5b_sft\"\n",
        "\n",
        "# ----------------------------------\n",
        "# 1) Load Tokenizer and Model\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Loading tokenizer and base model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"[OK] Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "print(\"[OK] Model loaded successfully!\")\n",
        "\n",
        "# ----------------------------------\n",
        "# 2) Load Training Dataset\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Loading training dataset...\")\n",
        "train_dataset = load_dataset(\"json\", data_files=train_data_file, split=\"train\")\n",
        "print(f\"[OK] Dataset loaded. Number of examples: {len(train_dataset)}\")\n",
        "\n",
        "# ----------------------------------\n",
        "# 3) PEFT/LoRA Configuration\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Configuring LoRA/PEFT...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"[OK] LoRA configuration applied!\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ----------------------------------\n",
        "# 4) SFT Training Setup (GPU Optimized)\n",
        "# ----------------------------------\n",
        "print(\"[INFO] Setting up SFT Trainer...\")\n",
        "training_args = SFTConfig(\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    output_dir=output_dir,\n",
        "    logging_steps=50,\n",
        "    save_steps=200,\n",
        "    bf16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "print(\"[OK] Trainer initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wACSCVUb4npE"
      },
      "source": [
        "## 5. Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kU4tGwJT4npE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "[INFO] Starting fine-tuning...\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [313/313 05:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.501942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.398805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.361538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.408564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.375305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.353803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=313, training_loss=2.3989940192371892, metrics={'train_runtime': 341.0938, 'train_samples_per_second': 29.347, 'train_steps_per_second': 0.918, 'total_flos': 1.78985834814336e+16, 'train_loss': 2.3989940192371892})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"[INFO] Starting fine-tuning...\")\n",
        "print(\"=\" * 60)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsrinpXq4npE"
      },
      "source": [
        "## 6. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ytHgmct64npE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Saving trained model...\n",
            "[DONE] Model saved to qwen2_0p5b_sft\n",
            "[INFO] Run metadata recorded to reports/run_metadata.jsonl\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] Saving trained model...\")\n",
        "trainer.save_model(output_dir)\n",
        "print(f\"[DONE] Model saved to {output_dir}\")\n",
        "\n",
        "# Record SFT training metadata\n",
        "import sys, json\n",
        "sys.path.insert(0, './src')\n",
        "from run_metadata import append_metadata\n",
        "append_metadata({\n",
        "    'type': 'sft_training',\n",
        "    'seed': 42,\n",
        "    'model_path': output_dir,\n",
        "    'train_data': train_data_file,\n",
        "    'num_samples': len(train_dataset),\n",
        "    'hyperparams': {\n",
        "        'learning_rate': 2e-4,\n",
        "        'num_train_epochs': 1,\n",
        "        'per_device_train_batch_size': 32,\n",
        "        'lora_r': 32,\n",
        "        'lora_alpha': 16,\n",
        "    },\n",
        "})\n",
        "print('[INFO] Run metadata recorded to reports/run_metadata.jsonl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQheWNL44npF"
      },
      "source": [
        "## 7. Download Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SFKqrWAF4npF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: qwen2_0p5b_sft/ (stored 0%)\n",
            "  adding: qwen2_0p5b_sft/training_args.bin (deflated 53%)\n",
            "  adding: qwen2_0p5b_sft/chat_template.jinja (deflated 71%)\n",
            "  adding: qwen2_0p5b_sft/README.md (deflated 44%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/ (stored 0%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/training_args.bin (deflated 53%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/chat_template.jinja (deflated 71%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/trainer_state.json (deflated 63%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/README.md (deflated 65%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/rng_state.pth (deflated 26%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/adapter_model.safetensors (deflated 7%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/tokenizer_config.json (deflated 59%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/optimizer.pt (deflated 8%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/tokenizer.json (deflated 81%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/adapter_config.json (deflated 56%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-200/scheduler.pt (deflated 61%)\n",
            "  adding: qwen2_0p5b_sft/adapter_model.safetensors (deflated 7%)\n",
            "  adding: qwen2_0p5b_sft/tokenizer_config.json (deflated 59%)\n",
            "  adding: qwen2_0p5b_sft/tokenizer.json (deflated 81%)\n",
            "  adding: qwen2_0p5b_sft/adapter_config.json (deflated 56%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/ (stored 0%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/training_args.bin (deflated 53%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/chat_template.jinja (deflated 71%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/trainer_state.json (deflated 65%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/README.md (deflated 65%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/rng_state.pth (deflated 26%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/adapter_model.safetensors (deflated 7%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/tokenizer_config.json (deflated 59%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/optimizer.pt (deflated 8%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/tokenizer.json (deflated 81%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/adapter_config.json (deflated 56%)\n",
            "  adding: qwen2_0p5b_sft/checkpoint-313/scheduler.pt (deflated 61%)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: stage1_sft.zip",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2229603566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip -r {output_dir}.zip {output_dir}/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'stage1_sft.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: stage1_sft.zip"
          ]
        }
      ],
      "source": [
        "# Package and download\n",
        "from google.colab import files\n",
        "!zip -r {output_dir}.zip {output_dir}/\n",
        "files.download(f'stage1_sft.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
